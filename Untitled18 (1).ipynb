{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV,GridSearchCV\n",
    "# nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import RepeatedKFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = pd.read_csv(\"C:/Users/sreel/Downloads/Data Science Data/Kaggle/sentiment analysis.csv\",encoding='latin-1', header=None)\n",
    "senti = senti[[0,5]]\n",
    "senti.columns = [\"Class\",\"Tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractions(s):\n",
    " s = re.sub(r\"won't\", 'will not',s)\n",
    " s = re.sub(r\"would't\", \"would not\",s)\n",
    " s = re.sub(r\"could't\", \"could not\",s)\n",
    " s = re.sub(r\"isn't\", \"is not\",s)\n",
    " s = re.sub(r\"\\'d\", \" would\",s)\n",
    " s = re.sub(r\"can\\'t\", \"can not\",s)\n",
    " s = re.sub(r\"n\\'t\", \" not\", s)\n",
    " s= re.sub(r\"\\'re\", \" are\", s)\n",
    " s = re.sub(r\"\\'s\", \" is\", s)\n",
    " s = re.sub(r\"\\'ll\", \" will\", s)\n",
    " s = re.sub(r\"\\'t\", \" not\", s)\n",
    " s = re.sub(r\"\\'ve\", \" have\", s)\n",
    " s = re.sub(r\"\\'m\", \" am\", s)\n",
    " return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in senti[[\"Tweet\"]].iterrows():\n",
    "    pattern1 = r'[^A-Za-z\\' ]+'\n",
    "    pattern2 = r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))';\n",
    "    pattern3 = r'@[\\w]*'\n",
    "    pattern4 = r'#[\\w]*'\n",
    "    text = re.sub(pattern2,'',row[\"Tweet\"])\n",
    "    text = re.sub(pattern3,'',text)\n",
    "    text = re.sub(pattern4,'',text)\n",
    "    text = re.sub(pattern1,'',text)\n",
    "    senti.at[index,'Tweet'] = text.lower()\n",
    "    \n",
    "senti['Tweet']=senti['Tweet'].apply(lambda x:contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "st = SnowballStemmer(language='english')\n",
    "\n",
    "def stem_text(text):\n",
    "    a=  ' '.join([st.stem(w) for w in w_tokenizer.tokenize(text)])\n",
    "    return a\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    a=  ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])\n",
    "    return a\n",
    "\n",
    "senti[['Tweet_Lemma']] = senti.Tweet.apply(stem_text)\n",
    "senti[['Tweet_Lemma']] = senti['Tweet_Lemma'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tf_x_train = vectorizer.fit_transform(senti[\"Tweet_Lemma\"])\n",
    "# tf_x_test = vectorizer.transform(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  alphas =  [0.05,0.06,0.07,0.08,0.09,0.1,0.002,0.004,0.006,0.008,0.009,0.2,0.3,0.4,0.5,0.6,0.7,0.8,\n",
    "#            0.9,0.003,0.005,0.007,0.04,0.001,0.01]\n",
    "#0.6, 0.7,0.9,0.8,0.4,1.0,1.1,1.2,1.3,1.4,1.5,1.6\n",
    "alphas =  [0.6, 0.7,0.9,0.8,0.4,1.0,1.1,1.2,1.3,1.4,1.5,1.6]\n",
    "# alphas = [0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01]\n",
    "clf_dts = pd.DataFrame(columns=[\"score\",\"alpha\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "log\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "log\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "log\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "log\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "log\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "log\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "log\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "log\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "log\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "log\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "log\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n",
      "split complete\n",
      "fit complete\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "kf = RepeatedKFold(n_splits=5, n_repeats=1, random_state=None) \n",
    "\n",
    "for c in alphas:\n",
    "    clf = LogisticRegression(max_iter=10000,C = c)\n",
    "    print(\"log\")\n",
    "    for train_index, test_index in kf.split(tf_x_train):\n",
    "        X_train, X_test = tf_x_train[train_index], tf_x_train[test_index] \n",
    "        y_train, y_test = senti[\"Class\"][train_index],senti[\"Class\"][test_index]\n",
    "        clf.fit(X_train,y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        a = []\n",
    "        a.append(score)\n",
    "        a.append(c)\n",
    "        clf_dts =  pd.concat([clf_dts,pd.DataFrame([a], columns=[\"score\",\"alpha\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.770141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>0.770526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>0.770417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.770467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>0.770381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.770539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.1</th>\n",
       "      <td>0.770411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <td>0.770612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.3</th>\n",
       "      <td>0.770362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.4</th>\n",
       "      <td>0.770349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.5</th>\n",
       "      <td>0.770200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.6</th>\n",
       "      <td>0.770140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          score\n",
       "alpha          \n",
       "0.4    0.770141\n",
       "0.6    0.770526\n",
       "0.7    0.770417\n",
       "0.8    0.770467\n",
       "0.9    0.770381\n",
       "1.0    0.770539\n",
       "1.1    0.770411\n",
       "1.2    0.770612\n",
       "1.3    0.770362\n",
       "1.4    0.770349\n",
       "1.5    0.770200\n",
       "1.6    0.770140"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_dts.groupby(by = [\"alpha\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,testX,trainY,testY = train_test_split(tf_x_train,senti[\"Class\"], test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.2, max_iter=500)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = LogisticRegression(max_iter=500,C = 1.2)\n",
    "logistic.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred=clf.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(testY, y_test_pred,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.8076206604572397,\n",
       "  'recall': 0.7749818745468636,\n",
       "  'f1-score': 0.7909647046815894,\n",
       "  'support': 239994},\n",
       " '4': {'precision': 0.7837328047095573,\n",
       "  'recall': 0.8154046148846279,\n",
       "  'f1-score': 0.7992550708479084,\n",
       "  'support': 240006},\n",
       " 'accuracy': 0.79519375,\n",
       " 'macro avg': {'precision': 0.7956767325833984,\n",
       "  'recall': 0.7951932447157457,\n",
       "  'f1-score': 0.7951098877647489,\n",
       "  'support': 480000},\n",
       " 'weighted avg': {'precision': 0.7956764339852016,\n",
       "  'recall': 0.79519375,\n",
       "  'f1-score': 0.7951099913943259,\n",
       "  'support': 480000}}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
